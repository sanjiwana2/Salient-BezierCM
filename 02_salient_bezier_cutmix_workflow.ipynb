{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import os\n",
    "from Model.salient_bezier_cutmix import salient_and_rect_dual_save_pipeline\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Number of augmented samples to generate\n",
    "n = 125\n",
    "\n",
    "# Define data directories\n",
    "Target_Drive = \"./Data/Forest\"       # Target class images (to be preserved & combined with masks)\n",
    "Source_Drive = \"./Data/Non_Forest\"    # Source class images (to be masked & pasted onto target)\n",
    "out_img_drive = \"./Data/Augmented_Image\"  # Output folder for augmented images\n",
    "out_label_drive = \"./Data/Augmented_Label\" # Output folder for augmented labels\n",
    "\n",
    "# Make sure output directories exist\n",
    "os.makedirs(out_img_drive, exist_ok=True)\n",
    "os.makedirs(out_label_drive, exist_ok=True)\n",
    "\n",
    "# Load pretrained EfficientNet-B4 model (ImageNet pretrained)\n",
    "model = models.efficientnet_b4(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Access the original first convolution layer\n",
    "# EfficientNet expects 3-channel input (RGB)\n",
    "orig_conv = model.features[0][0]   # nn.Conv2d(3, 48, ...)\n",
    "\n",
    "# Create a new convolution layer that accepts 4 input channels instead of 3\n",
    "new_conv = nn.Conv2d(\n",
    "    in_channels=4,  # now supports 4-band input (e.g., RGB + NIR)\n",
    "    out_channels=orig_conv.out_channels,\n",
    "    kernel_size=orig_conv.kernel_size,\n",
    "    stride=orig_conv.stride,\n",
    "    padding=orig_conv.padding,\n",
    "    bias=orig_conv.bias is not None\n",
    ")\n",
    "\n",
    "# Copy pretrained weights for the first 3 channels (RGB)\n",
    "# Initialize the 4th channel (NIR) with small random weights\n",
    "with torch.no_grad():\n",
    "    new_conv.weight[:, :3, :, :] = orig_conv.weight\n",
    "    new_conv.weight[:, 3:, :, :] = torch.randn_like(new_conv.weight[:, 3:, :, :]) * 0.01\n",
    "\n",
    "# Replace the classifier head for binary classification (forest vs non-forest)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "# Replace the first conv layer with our modified 4-channel version\n",
    "model.features[0][0] = new_conv\n",
    "\n",
    "# Load pretrained weights (fine-tuned on forest vs non-forest task)\n",
    "model.load_state_dict(torch.load(\"efficientb4_2k_forest_nonforest.pth\"))\n",
    "\n",
    "# Run the salient CutMix augmentation pipeline\n",
    "# This will create new augmented samples by blending\n",
    "# target (forest) and source (non-forest) images guided by saliency/mask\n",
    "salient_and_rect_dual_save_pipeline(\n",
    "    model=model,\n",
    "    target_drive=Target_Drive,\n",
    "    source_drive=Source_Drive,\n",
    "    out_img_drive=out_img_drive,\n",
    "    out_label_drive=out_label_drive,\n",
    "    n=n,\n",
    "    device=DEVICE,\n",
    "    prefix = 'forest_as_target',\n",
    "    inverse=True\n",
    ")\n",
    "\n",
    "# Do the inversion of first step\n",
    "salient_and_rect_dual_save_pipeline(\n",
    "    model=model,\n",
    "    target_drive=Source_Drive,\n",
    "    source_drive=Target_Drive,\n",
    "    out_img_drive=out_img_drive,\n",
    "    out_label_drive=out_label_drive,\n",
    "    n=n,\n",
    "    device=DEVICE,\n",
    "    prefix = 'non_forest_as_target',\n",
    "    inverse =None\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
